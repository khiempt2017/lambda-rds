{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the basic Glue, Spark libraries \n",
    "\n",
    "import os, sys, boto3\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "# Important further required libraries\n",
    "from pprint import pprint\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Starting Spark/Glue Context\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"medicine_time1\", StringType(), True),\n",
    "    StructField(\"medicine_time2\", StringType(), True),\n",
    "    StructField(\"medicine_time3\", StringType(), True),\n",
    "    StructField(\"memo_mask\", StringType(), True),\n",
    "    StructField(\"memo_text\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"updated_at\", StringType(), True)\n",
    "])\n",
    "# AWS configuration\n",
    "s3_bucket_name = \"s3://dynamodb-csv-importing/diaries/\"\n",
    "region_name = 'ap-northeast-1'\n",
    "ddb_table_name = 'BPDiary-diaries_performance'\n",
    "\n",
    "# List of files to import\n",
    "file_list = [\n",
    "    \"diaries-dummy-1.csv\", \"diaries-dummy-10.csv\", \"diaries-dummy-11.csv\", \"diaries-dummy-12.csv\", \"diaries-dummy-13.csv\", \"diaries-dummy-14.csv\", \"diaries-dummy-15.csv\", \"diaries-dummy-16.csv\", \"diaries-dummy-17.csv\", \"diaries-dummy-18.csv\", \"diaries-dummy-19.csv\", \"diaries-dummy-2.csv\", \"diaries-dummy-20.csv\", \"diaries-dummy-21.csv\", \"diaries-dummy-22.csv\", \"diaries-dummy-23.csv\", \"diaries-dummy-24.csv\", \"diaries-dummy-25.csv\", \"diaries-dummy-3.csv\", \"diaries-dummy-4-1.csv\", \"diaries-dummy-4.csv\", \"diaries-dummy-5.csv\", \"diaries-dummy-6.csv\", \"diaries-dummy-7.csv\", \"diaries-dummy-8.csv\", \"diaries-dummy-9.csv\", \"diaries.csv\"\n",
    "]\n",
    "\n",
    "# Read each file and union them into a single DataFrame\n",
    "df_list = []\n",
    "for file_name in file_list:\n",
    "    df = spark.read.load(s3_bucket_name + file_name, \n",
    "                         format=\"csv\", \n",
    "                         sep=\",\", \n",
    "                         inferSchema=\"true\",\n",
    "                         schema=schema,\n",
    "                         header=\"true\")\n",
    "    df_list.append(df)\n",
    "\n",
    "df = df_list[0]\n",
    "for temp_df in df_list[1:]:\n",
    "    df = df.union(temp_df)\n",
    "\n",
    "# transform DataFrame into DynamicFrame\n",
    "df_dyf = DynamicFrame.fromDF(df, glueContext, \"df_dyf\")\n",
    "\n",
    "# write data to DynamoDB\n",
    "print(\"Start writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "glueContext.write_dynamic_frame_from_options(\n",
    "    frame=df_dyf,\n",
    "    connection_type=\"dynamodb\",\n",
    "    connection_options={\n",
    "        \"dynamodb.output.tableName\": ddb_table_name,\n",
    "        \"dynamodb.throughput.write.percent\": \"1.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Schema of DataFrame: {df.printSchema()}\")\n",
    "print(f\"Preview of DataFrame: {df.show(5)}\")\n",
    "\n",
    "print(\"Finished writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "# count data\n",
    "print(f\"Number of records written: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
