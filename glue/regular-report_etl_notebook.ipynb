{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Importing all the basic Glue, Spark libraries \n",
        "\n",
        "import os, sys\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "# Important further required libraries\n",
        "from awsglue.dynamicframe import DynamicFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from datetime import datetime\n",
        "\n",
        "# Starting Spark/Glue Context\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"refresh_token\", StringType(), True),\n",
        "    StructField(\"token_tmp\", StringType(), True),\n",
        "    StructField(\"login_flg\", IntegerType(), True),\n",
        "    StructField(\"logout_flg\", IntegerType(), True),\n",
        "    StructField(\"init_tutorial\", IntegerType(), True),\n",
        "    StructField(\"introduction\", IntegerType(), True),\n",
        "    StructField(\"email_slide_tutorial\", IntegerType(), True),\n",
        "    StructField(\"first_time_set_nickname\", IntegerType(), True),\n",
        "    StructField(\"auto_send\", IntegerType(), True),\n",
        "    StructField(\"message_flg\", IntegerType(), True),\n",
        "    StructField(\"created_at\", StringType(), True),\n",
        "    StructField(\"updated_at\", StringType(), True)\n",
        "])\n",
        "# AWS configuration\n",
        "s3_bucket_name = \"s3://dynamodb-csv-importing/regular_report/\"\n",
        "ddb_table_name = 'BPDiary-regular_report_performance'\n",
        "\n",
        "# Read file from S3\n",
        "file_list = [\n",
        "    \"regular_report.csv\", \"regular_report-dummy.csv\"\n",
        "]\n",
        "\n",
        "# Read each file and union them into a single DataFrame\n",
        "df_list = []\n",
        "for file_name in file_list:\n",
        "    df = spark.read.load(s3_bucket_name + file_name, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\",\n",
        "                         schema=schema,\n",
        "                         header=\"true\")\n",
        "    df_list.append(df)\n",
        "\n",
        "df = df_list[0]\n",
        "for temp_df in df_list[1:]:\n",
        "    df = df.union(temp_df)\n",
        "\n",
        "# transform DataFrame into DynamicFrame\n",
        "df_dyf = DynamicFrame.fromDF(df, glueContext, \"df_dyf\")\n",
        "\n",
        "# write data to DynamoDB\n",
        "print(\"Start writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "glueContext.write_dynamic_frame_from_options(\n",
        "    frame=df_dyf,\n",
        "    connection_type=\"dynamodb\",\n",
        "    connection_options={\n",
        "        \"dynamodb.output.tableName\": ddb_table_name,\n",
        "        \"dynamodb.throughput.write.percent\": \"1.0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Schema of DataFrame: {df.printSchema()}\")\n",
        "print(f\"Preview of DataFrame: {df.show(5)}\")\n",
        "\n",
        "print(\"Finished writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "# count data\n",
        "print(f\"Number of records written: {df.count()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
