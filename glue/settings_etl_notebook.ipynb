{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Importing all the basic Glue, Spark libraries \n",
        "\n",
        "import os, sys\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "# Important further required libraries\n",
        "from awsglue.dynamicframe import DynamicFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from datetime import datetime\n",
        "\n",
        "# Starting Spark/Glue Context\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"systolic_target\", IntegerType(), True),\n",
        "    StructField(\"diastolic_target\", IntegerType(), True),\n",
        "    StructField(\"medicine_per_day\", IntegerType(), True),\n",
        "    StructField(\"medicine_alarm_time1\", StringType(), True),\n",
        "    StructField(\"medicine_alarm_time2\", StringType(), True),\n",
        "    StructField(\"medicine_alarm_time3\", StringType(), True),\n",
        "    StructField(\"medicine_alarm_state\", IntegerType(), True),\n",
        "    StructField(\"bp_morning_start\", StringType(), True),\n",
        "    StructField(\"bp_morning_end\", StringType(), True),\n",
        "    StructField(\"bp_evening_start\", StringType(), True),\n",
        "    StructField(\"bp_evening_end\", StringType(), True),\n",
        "    StructField(\"graph_type\", IntegerType(), True),\n",
        "    StructField(\"bp_scale_min_def\", IntegerType(), True),\n",
        "    StructField(\"bp_scale_max_def\", IntegerType(), True),\n",
        "    StructField(\"graph_mask\", IntegerType(), True),\n",
        "    StructField(\"initial_setting_done\", IntegerType(), True),\n",
        "    StructField(\"tutorial_done_mask\", IntegerType(), True),\n",
        "    StructField(\"memo_mask\", IntegerType(), True),\n",
        "    StructField(\"medal_3_days\", IntegerType(), True),\n",
        "    StructField(\"medal_5_days\", IntegerType(), True),\n",
        "    StructField(\"medal_7_days\", IntegerType(), True),\n",
        "    StructField(\"medal_14_days\", IntegerType(), True),\n",
        "    StructField(\"medal_30_days\", IntegerType(), True),\n",
        "    StructField(\"finish_initial\", IntegerType(), True),\n",
        "    StructField(\"show_tutorial\", IntegerType(), True),\n",
        "    StructField(\"on_boarding\", StringType(), True),\n",
        "    StructField(\"finish_register\", IntegerType(), True),\n",
        "    StructField(\"created_at\", StringType(), True),\n",
        "    StructField(\"updated_at\", StringType(), True),\n",
        "    StructField(\"email_popup_displayed\", IntegerType(), True),\n",
        "    StructField(\"memo_icon_popup_displayed\", IntegerType(), True),\n",
        "    StructField(\"medicine_register\", IntegerType(), True),\n",
        "    StructField(\"logbook_separation_popup_displayed\", IntegerType(), True),\n",
        "    StructField(\"premium_popup_displayed\", IntegerType(), True),\n",
        "    StructField(\"how_to_graph_popup_displayed\", IntegerType(), True)\n",
        "])\n",
        "# AWS configuration\n",
        "s3_bucket_name = \"s3://dynamodb-csv-importing/settings/\"\n",
        "ddb_table_name = 'BPDiary-settings_performance'\n",
        "\n",
        "# Read file from S3\n",
        "file_list = [\n",
        "    \"settings.csv\", \"settings-dummy.csv\"\n",
        "]\n",
        "\n",
        "# Read each file and union them into a single DataFrame\n",
        "df_list = []\n",
        "for file_name in file_list:\n",
        "    df = spark.read.load(s3_bucket_name + file_name, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\",\n",
        "                         schema=schema,\n",
        "                         header=\"true\")\n",
        "    df_list.append(df)\n",
        "\n",
        "df = df_list[0]\n",
        "for temp_df in df_list[1:]:\n",
        "    df = df.union(temp_df)\n",
        "\n",
        "# transform DataFrame into DynamicFrame\n",
        "df_dyf = DynamicFrame.fromDF(df, glueContext, \"df_dyf\")\n",
        "\n",
        "# write data to DynamoDB\n",
        "print(\"Start writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "glueContext.write_dynamic_frame_from_options(\n",
        "    frame=df_dyf,\n",
        "    connection_type=\"dynamodb\",\n",
        "    connection_options={\n",
        "        \"dynamodb.output.tableName\": ddb_table_name,\n",
        "        \"dynamodb.throughput.write.percent\": \"1.0\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Schema of DataFrame: {df.printSchema()}\")\n",
        "print(f\"Preview of DataFrame: {df.show(5)}\")\n",
        "\n",
        "print(\"Finished writing to DynamoDB: {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
        "# count data\n",
        "print(f\"Number of records written: {df.count()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
